/**
 * Created by shrinidhihudli on 2/6/15.
 *
 * --This script tests a join too large for fragment and replicate.  It also
 * --contains a join followed by a group by on the same key, something that we
 * --could potentially optimize by not regrouping.
 * register $PIGMIX_JAR
 * A = load '$HDFS_ROOT/page_views' using org.apache.pig.test.pigmix.udf.PigPerformanceLoader()
 *   as (user, action, timespent, query_term, ip_addr, timestamp,
 *       estimated_revenue, page_info, page_links);
 * B = foreach A generate user, (double)estimated_revenue;
 * alpha = load '$HDFS_ROOT/users' using PigStorage('\u0001') as (name, phone, address,
 *       city, state, zip);
 * beta = foreach alpha generate name;
 * C = join beta by name, B by user parallel $PARALLEL;
 * D = group C by $0 parallel $PARALLEL;
 * E = foreach D generate group, SUM(C.estimated_revenue);
 * store E into '$PIGMIX_OUTPUT/L3out';
 */
package org.apache.spark.examples.sparkmix

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import java.util.Properties
import java.io.FileInputStream

import org.apache.spark.lineage.LineageContext

object L3 {
  def run(sc: SparkContext, lc: LineageContext, pigMixPath: String, outputPath: String): Long = {

    val properties: Properties = SparkMixUtils.loadPropertiesFile()

    val pageViewsPath = pigMixPath + "page_views/"
    val usersPath = pigMixPath + "users/"

    var pageViews = sc.textFile(pageViewsPath)
    var users = sc.textFile(usersPath)

    if (lc != null) {
      pageViews = lc.textFile(pageViewsPath)
      users = lc.textFile(usersPath)
      lc.setCaptureLineage(true)
    }

    val start = System.currentTimeMillis()

    val A = pageViews.map(x => (SparkMixUtils.safeSplit(x, "\u0001", 0), SparkMixUtils.safeSplit(x, "\u0001", 1),
      SparkMixUtils.safeSplit(x, "\u0001", 2), SparkMixUtils.safeSplit(x, "\u0001", 3),
      SparkMixUtils.safeSplit(x, "\u0001", 4), SparkMixUtils.safeSplit(x, "\u0001", 5),
      SparkMixUtils.safeSplit(x, "\u0001", 6),
      SparkMixUtils.createMap(SparkMixUtils.safeSplit(x, "\u0001", 7)),
      SparkMixUtils.createBag(SparkMixUtils.safeSplit(x, "\u0001", 8))))

    val B = A.map(x => (x._1, SparkMixUtils.safeDouble(x._7)))

    val alpha = users.map(x => (SparkMixUtils.safeSplit(x, "\u0001", 0), SparkMixUtils.safeSplit(x, "\u0001", 1),
      SparkMixUtils.safeSplit(x, "\u0001", 2), SparkMixUtils.safeSplit(x, "\u0001", 3),
      SparkMixUtils.safeSplit(x, "\u0001", 4)))

    val beta = alpha.map(x => (x._1, 1))

    val C = B.join(beta, properties.getProperty("PARALLEL").toInt).map(x => (x._1, x._1, x._2._1))

    val D = C.groupBy(_._1) //TODO add $PARALLEL

    val E = D.map(x => (x._1, x._2.reduce((a, b) => (a._1 + b._1, a._2 + b._2, a._3 + b._3)))).map(x => (x._1, x._2._3))

    val end = System.currentTimeMillis()

    if (lc != null)
      lc.setCaptureLineage(false)

    E.saveAsTextFile(outputPath)

    return (end - start)
  }
}
